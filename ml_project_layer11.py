# -*- coding: utf-8 -*-
"""ML_Project_Layer11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EQoNecrsGIck6hRkW0kI0O0WDQC-w8JM
"""

import pandas as pd
import numpy as np

from google.colab import drive
MOUNT_PATH='/content/drive'
drive.mount(MOUNT_PATH)

WORKING_DIR=f"{MOUNT_PATH}/MyDrive/ML_Project/Layer11"

WORKING_DIR="layer-11-new"

"""### Importing dataset"""

train = pd.read_csv(f"{WORKING_DIR}/layer_11_train.csv")
train.head()

valid = pd.read_csv(f"{WORKING_DIR}/layer_11_valid.csv")
valid.head()

test = pd.read_csv(f"{WORKING_DIR}/layer_11_test.csv")
test.head()

# Checking for missing values
print(train.isnull().sum())

#constants
L1 = 'label_1'
L2 = 'label_2'
L3 = 'label_3'
L4 = 'label_4'

LABELS = [L1, L2, L3, L4]
FEATURES = [f"feature_{i}" for i in range (1,769)]
ID = "ID"

print(LABELS)
print(FEATURES)

import matplotlib.pyplot as plt

# Plotting each label
fig, axs = plt.subplots(2, 2, figsize=(14, 10))

for idx, label in enumerate(LABELS):
    ax = axs[idx // 2, idx % 2]  # Determines the position of the subplot
    train[label].value_counts().plot(kind='bar', ax=ax)
    ax.set_title(f'Distribution of {label}')
    ax.set_ylabel('Count')
    ax.set_xlabel(label)

plt.tight_layout()
plt.show()

x_train = {}
y_train = {}
x_valid = {}
y_valid = {}
x_test = {}
y_test = {}

"""### **Data Preprocess**


*   Drop NaN from label 2
*   Random oversample label 4
"""

#!pip install imbalanced-learn

from imblearn.over_sampling import RandomOverSampler
from collections import Counter

# Initialize the RandomOverSampler
ros = RandomOverSampler(random_state=42)

from sklearn.preprocessing import RobustScaler

for target_label in LABELS:
  train_ds = train[train['label_2'].notna()] if target_label == 'label_2' else train
  valid_ds = valid[valid['label_2'].notna()] if target_label == 'label_2' else valid
  test_ds = test[test['label_2'].notna()] if target_label == 'label_2' else test

  scaler = RobustScaler()
  x_train[target_label] = pd.DataFrame(scaler.fit_transform(train_ds.drop(LABELS, axis = 1)), columns=FEATURES)
  y_train[target_label] = train_ds[target_label]

  x_valid[target_label] = pd.DataFrame(scaler.transform(valid_ds.drop(LABELS, axis = 1)), columns=FEATURES)
  y_valid[target_label] = valid_ds[target_label]

  x_test[target_label] = pd.DataFrame(scaler.transform(test_ds.drop(LABELS, axis = 1)), columns=FEATURES)

# # Apply random oversampling specifically when target_label is 'label_4'
# for target_label in LABELS:
#     train_ds = train[train['label_2'].notna()] if target_label == 'label_2' else train
#     valid_ds = valid[valid['label_2'].notna()] if target_label == 'label_2' else valid
#     test_ds = test

#     # Apply Random Oversampling only for label_4 (Accent)
#     if target_label == 'label_4':
#         # Print original class distribution
#         print("Original class distribution:", Counter(train_ds['label_4']))

#         # Perform Random OverSampling
#         X_resampled, y_resampled = ros.fit_resample(train_ds[FEATURES], train_ds['label_4'])

#         # Create a new DataFrame from the resampled data
#         train_resampled = pd.DataFrame(X_resampled, columns=FEATURES)
#         train_resampled['label_4'] = y_resampled

#         # If other labels are needed, merge them here
#         other_labels = train_ds[LABELS].drop('label_4', axis=1).reset_index(drop=True)
#         train_resampled = pd.concat([train_resampled, other_labels], axis=1)

#         # Print new class distribution
#         print("New class distribution:", Counter(train_resampled['label_4']))

#         # Use train_resampled for subsequent steps involving label_4
#         train_ds = train_resampled

#     scaler = RobustScaler()
#     x_train[target_label] = pd.DataFrame(scaler.fit_transform(train_ds[FEATURES]), columns=FEATURES)
#     y_train[target_label] = train_ds[target_label]

#     x_valid[target_label] = pd.DataFrame(scaler.transform(valid_ds[FEATURES]), columns=FEATURES)
#     y_valid[target_label] = valid_ds[target_label]

import matplotlib.pyplot as plt

# Plotting each label
fig, axs = plt.subplots(2, 2, figsize=(14, 10))

for idx, label in enumerate(LABELS):
    ax = axs[idx // 2, idx % 2]  # Determines the position of the subplot
    train_ds[label].value_counts().plot(kind='bar', ax=ax)
    ax.set_title(f'Distribution of {label}')
    ax.set_ylabel('Count')
    ax.set_xlabel(label)

plt.tight_layout()
plt.show()

# x_test = pd.DataFrame(scaler.transform(test_ds.drop(ID, axis =1)), columns=FEATURES)
y_test = test_ds

id = test_ds[ID]

id

"""# Label 1"""

from sklearn import svm

clf_L1 = svm.SVC(kernel = 'linear')
clf_L1.fit(x_train[L1], y_train[L1])

from sklearn import metrics

y_pred_L1 = clf_L1.predict(x_valid[L1])
y_pred_test_before_L1 = clf_L1.predict(x_test[L1])

print('Predicted labels before feature engineering:', y_pred_test_before_L1)

print (metrics.confusion_matrix(y_valid[L1], y_pred_L1))
print (metrics.accuracy_score(y_valid[L1], y_pred_L1))
print (metrics.precision_score(y_valid[L1], y_pred_L1, average="weighted"))
print (metrics.recall_score(y_valid[L1], y_pred_L1, average="weighted"))

"""## Feature Engineering

### PCA
"""

from sklearn.decomposition import PCA

pca_L1 = PCA(n_components=0.95, svd_solver='full')
pca_L1.fit(x_train[L1])
x_train_trf_L1 = pd.DataFrame(pca_L1.transform(x_train[L1]))
x_valid_trf_L1 = pd.DataFrame(pca_L1.transform(x_valid[L1]))

print("Shape after PCA: ", x_train_trf_L1.shape)

x_test_trf_L1 = pd.DataFrame(pca_L1.transform(x_test[L1]))

clf_L1 = svm.SVC(kernel = 'linear')
clf_L1.fit(x_train_trf_L1, y_train[L1])

y_pred_L1 = clf_L1.predict(x_valid_trf_L1)

print (metrics.confusion_matrix(y_valid[L1], y_pred_L1))
print (metrics.accuracy_score(y_valid[L1], y_pred_L1))
print (metrics.precision_score(y_valid[L1], y_pred_L1, average="weighted"))
print (metrics.recall_score(y_valid[L1], y_pred_L1, average="weighted"))

"""### PCA with Hyper-parameter Tuning and Cross Validation"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score

# Define the model
svc_L1 = svm.SVC()

# Parameters to search
param_distributions_L1 = {
    'C': np.logspace(-3, 3, 7),  # logarithmic scale from 0.001 to 1000
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': np.logspace(-3, 3, 7)
}

"""**Wrong**"""

# Randomized search
random_search_L1 = RandomizedSearchCV(svc_L1, param_distributions_L1, n_iter=5, cv=5, scoring='accuracy', random_state=42, verbose=2, n_jobs=-1)
random_search_L1.fit(x_train[L1], y_train[L1])

print(f"Best parameters: {random_search_L1.best_params_}")
print(f"Best cross-validation score: {random_search_L1.best_score_}")
print(f"Best model: {random_search_L1.best_estimator_}")

best_params = random_search_L1.best_params_
best_model = random_search_L1.best_estimator_

y_pred = best_model.predict(x_valid[L1])
accuracy = accuracy_score(y_valid[L1], y_pred)
print(f'Accuracy on test data:{accuracy}')

y_pred_L1 = best_model.predict(x_test)

y_pred_L1

"""### Correct"""

# Randomized search new
random_search_L1_new = RandomizedSearchCV(svc_L1, param_distributions_L1, n_iter=5, cv=5, scoring='accuracy', random_state=42, verbose=2, n_jobs=-1)
random_search_L1_new.fit(x_train_trf_L1, y_train[L1])

print(f"Best parameters: {random_search_L1_new.best_params_}")
print(f"Best cross-validation score: {random_search_L1_new.best_score_}")
print(f"Best model: {random_search_L1_new.best_estimator_}")

best_params_L1 = random_search_L1_new.best_params_
best_model_L1 = random_search_L1_new.best_estimator_

y_pred = best_model_L1.predict(x_valid_trf_L1)
accuracy = accuracy_score(y_valid[L1], y_pred)
print(f'Accuracy on test data:{accuracy}')

y_pred_L1 = best_model_L1.predict(x_test_trf_L1)
y_pred_L1

"""Test"""

test_L1 = svm.SVC(C=10.0, class_weight='balanced', gamma=0.01, kernel='linear')
test_L1.fit(x_train_trf_L1, y_train[L1])

y_pred_L1_test = test_L1.predict(x_valid_trf_L1)
print (metrics.accuracy_score(y_valid[L1], y_pred_L1_test))

"""Save the best model"""

import joblib

# Save the model to your Google Drive using joblib
joblib.dump(best_model_L1, f'{WORKING_DIR}/best_model_L1.joblib')

loaded_model = joblib.load(f'{WORKING_DIR}/best_model_L1.joblib')

y_pred_L1_load = loaded_model.predict(x_valid_trf_L1)
accuracy = metrics.accuracy_score(y_valid[L1], y_pred_L1_load)
print(f'Accuracy on test data: {accuracy}')

print (metrics.confusion_matrix(y_valid[L1], y_pred_L1_load))
print (metrics.accuracy_score(y_valid[L1], y_pred_L1_load))
print (metrics.precision_score(y_valid[L1], y_pred_L1_load, average="weighted"))
print (metrics.recall_score(y_valid[L1], y_pred_L1_load, average="weighted"))

# Create the base DataFrame
output_df_L1 = pd.DataFrame({
    'label_1': y_pred_L1_load
})

output_df_L1.head()

output_df_L1.to_csv(f"{WORKING_DIR}/L1.csv", index=False)

"""# Label 2"""

from sklearn import svm

clf_L2 = svm.SVC(kernel = 'linear', class_weight='balanced')
clf_L2.fit(x_train[L2], y_train[L2])

from sklearn import metrics

y_pred_L2 = clf_L2.predict(x_valid[L2])
y_pred_test_before_L2 = clf_L2.predict(x_test[L2])

print('Predicted labels before feature engineering:', y_pred_test_before_L2)

print (metrics.confusion_matrix(y_valid[L2], y_pred_L2))
print (metrics.accuracy_score(y_valid[L2], y_pred_L2))
print (metrics.precision_score(y_valid[L2], y_pred_L2, average="weighted"))
print (metrics.recall_score(y_valid[L2], y_pred_L2, average="weighted"))

"""## Feature Engineering"""

from sklearn.decomposition import PCA

pca_L2 = PCA(n_components=0.95, svd_solver='full')
pca_L2.fit(x_train[L2])
x_train_trf_L2 = pd.DataFrame(pca_L2.transform(x_train[L2]))
x_valid_trf_L2 = pd.DataFrame(pca_L2.transform(x_valid[L2]))

print("Shape after PCA: ", x_train_trf_L2.shape)

x_test_trf_L2= pd.DataFrame(pca_L2.transform(x_test[L2]))

from sklearn import svm
from sklearn import metrics

clf_L2 = svm.SVC(kernel = 'linear', class_weight='balanced')
clf_L2.fit(x_train_trf_L2, y_train[L2])

y_pred_L2 = clf_L2.predict(x_valid_trf_L2)

print (metrics.confusion_matrix(y_valid[L2], y_pred_L2))
print (metrics.accuracy_score(y_valid[L2], y_pred_L2))
print (metrics.precision_score(y_valid[L2], y_pred_L2, average="weighted"))
print (metrics.recall_score(y_valid[L2], y_pred_L2, average="weighted"))

"""### PCA with Hyper-parameter Tuning and Cross Validation


"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score

# Define the model
svc_L2 = svm.SVC(class_weight='balanced')

# Parameters to search
param_distributions_L2 = {
    'C': np.logspace(-3, 3, 7),  # logarithmic scale from 0.001 to 1000
    'kernel': ['poly'],
    'gamma': np.logspace(-3, 3, 7)
}

"""**Wrong**"""

# Randomized search
random_search_L2 = RandomizedSearchCV(svc_L2, param_distributions_L2, n_iter=5, cv=5, scoring='accuracy', random_state=42, verbose=2, n_jobs=-1)
random_search_L2.fit(x_train[L2], y_train[L2])

print(f"Best parameters: {random_search_L2.best_params_}")
print(f"Best cross-validation score: {random_search_L2.best_score_}")
print(f"Best model: {random_search_L2.best_estimator_}")

best_params = random_search_L2.best_params_
best_model = random_search_L2.best_estimator_

y_pred = best_model.predict(x_valid[L2])
accuracy = accuracy_score(y_valid[L2], y_pred)
print(f'Accuracy on test data:{accuracy}')

y_pred_L2 = best_model.predict(x_test)

y_pred_L2

"""Correct"""

# Randomized search new
random_search_L2_new = RandomizedSearchCV(svc_L2, param_distributions_L2, n_iter=3, cv=3, scoring='accuracy', random_state=42, verbose=2)
random_search_L2_new.fit(x_train_trf_L2, y_train[L2])

print(f"Best parameters: {random_search_L2_new.best_params_}")
print(f"Best cross-validation score: {random_search_L2_new.best_score_}")
print(f"Best model: {random_search_L2_new.best_estimator_}")

best_params_L2 = random_search_L2_new.best_params_
best_model_L2 = random_search_L2_new.best_estimator_

y_pred = best_model_L2.predict(x_valid_trf_L2)
accuracy = accuracy_score(y_valid[L2], y_pred)
print(f'Accuracy on test data:{accuracy}')

y_pred_L2 = best_model_L2.predict(x_test_trf_L2)
y_pred_L2

"""Test"""

test_L2 = svm.SVC(C=10.0, class_weight='balanced', gamma=0.001, kernel='rbf')
test_L2.fit(x_train_trf_L2, y_train[L2])

y_pred_L2_test = test_L2.predict(x_valid_trf_L2)
print (metrics.accuracy_score(y_valid[L2], y_pred_L2_test))

test_L2 = svm.SVC(C=100.0, class_weight='balanced', gamma=0.001, kernel='rbf')
test_L2.fit(x_train_trf_L2, y_train[L2])

y_pred_L2_test = test_L2.predict(x_valid_trf_L2)
print (metrics.accuracy_score(y_valid[L2], y_pred_L2_test))

test_L2 = svm.SVC(C=1000.0, class_weight='balanced', gamma=0.001, kernel='rbf')
test_L2.fit(x_train_trf_L2, y_train[L2])

y_pred_L2_test = test_L2.predict(x_valid_trf_L2)
print (metrics.accuracy_score(y_valid[L2], y_pred_L2_test))

test_L2 = svm.SVC(C=100.0, class_weight='balanced', gamma=0.01, kernel='rbf')
test_L2.fit(x_train_trf_L2, y_train[L2])

y_pred_L2_test = test_L2.predict(x_valid_trf_L2)
print (metrics.accuracy_score(y_valid[L2], y_pred_L2_test))

"""Save the best model"""

import joblib

test_L2 = svm.SVC(C=1000.0, class_weight='balanced', gamma=0.001, kernel='rbf')
test_L2.fit(x_train_trf_L2, y_train[L2])
joblib.dump(test_L2, f'{WORKING_DIR}/best_model_L2_new.joblib')

loaded_model = joblib.load(f'{WORKING_DIR}/best_model_L2_new.joblib')
y_pred_L2_load = loaded_model.predict(x_valid_trf_L2)
accuracy = metrics.accuracy_score(y_valid[L2], y_pred_L2_load)
print(f'Accuracy on test data: {accuracy}')

# Save the model to your Google Drive using joblib
joblib.dump(best_model_L2, f'{WORKING_DIR}/best_model_L2.joblib')

loaded_model = joblib.load(f'{WORKING_DIR}/best_model_L2.joblib')

y_pred_L2_load = loaded_model.predict(x_valid_trf_L2)
accuracy = accuracy_score(y_valid[L2], y_pred_L2_load)
print(f'Accuracy on test data: {accuracy}')

print (metrics.confusion_matrix(y_valid[L2], y_pred_L2_load))
print (metrics.accuracy_score(y_valid[L2], y_pred_L2_load))
print (metrics.precision_score(y_valid[L2], y_pred_L2_load, average="weighted"))
print (metrics.recall_score(y_valid[L2], y_pred_L2_load, average="weighted"))

# Create the base DataFrame
output_df_L2 = pd.DataFrame({
    'label_2': y_pred_L2_load
})

output_df_L2.head()

output_df_L2.to_csv(f"{WORKING_DIR}/L2.csv", index=False)

"""# Label 3"""

from sklearn import svm

clf_L3 = svm.SVC(kernel = 'linear', class_weight='balanced')
clf_L3.fit(x_train[L3], y_train[L3])

from sklearn import metrics

y_pred_L3 = clf_L3.predict(x_valid[L3])
y_pred_test_before_L3 = clf_L3.predict(x_test[L3])

print('Predicted labels before feature engineering:', y_pred_test_before_L3)

print (metrics.confusion_matrix(y_valid[L3], y_pred_L3))
print (metrics.accuracy_score(y_valid[L3], y_pred_L3))
print (metrics.precision_score(y_valid[L3], y_pred_L3, average="weighted"))
print (metrics.recall_score(y_valid[L3], y_pred_L3, average="weighted"))

"""## Feature Engineering"""

from sklearn.decomposition import PCA

pca_L3 = PCA(n_components=0.95, svd_solver='full')
pca_L3.fit(x_train[L3])
x_train_trf_L3 = pd.DataFrame(pca_L3.transform(x_train[L3]))
x_valid_trf_L3 = pd.DataFrame(pca_L3.transform(x_valid[L3]))

print("Shape after PCA: ", x_train_trf_L3.shape)

x_test_trf_L3= pd.DataFrame(pca_L3.transform(x_test[L3]))

clf_L3 = svm.SVC(kernel = 'linear', class_weight='balanced')
clf_L3.fit(x_train_trf_L3, y_train[L3])

y_pred_L3 = clf_L3.predict(x_valid_trf_L3)

print (metrics.confusion_matrix(y_valid[L3], y_pred_L3))
print (metrics.accuracy_score(y_valid[L3], y_pred_L3))
print (metrics.precision_score(y_valid[L3], y_pred_L3, average="weighted"))
print (metrics.recall_score(y_valid[L3], y_pred_L3, average="weighted"))

"""### PCA with Hyper-parameter Tuning and Cross Validation

"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score

# Define the model
svc_L3 = svm.SVC(class_weight='balanced')

# Parameters to search
param_distributions_L3 = {
    'C': np.logspace(-2, 2, 4),  # logarithmic scale from 0.001 to 1000
    'kernel': ['linear'],
    'gamma': np.logspace(-2, 2, 4)
}

"""**Wrong**"""

# Randomized search
random_search_L3 = RandomizedSearchCV(svc_L3, param_distributions_L3, n_iter=5, cv=5, scoring='accuracy', random_state=42, verbose=2, n_jobs=-1)
random_search_L3.fit(x_train[L3], y_train[L3])

print(f"Best parameters: {random_search_L3.best_params_}")
print(f"Best cross-validation score: {random_search_L3.best_score_}")
print(f"Best model: {random_search_L3.best_estimator_}")

best_params = random_search_L3.best_params_
best_model = random_search_L3.best_estimator_

y_pred = best_model.predict(x_valid[L3])
accuracy = accuracy_score(y_valid[L3], y_pred)
print(f'Accuracy on test data:{accuracy}')

y_pred_L3 = best_model.predict(x_test)

y_pred_L3

"""Correct"""

# Randomized search new
random_search_L3_new = RandomizedSearchCV(svc_L3, param_distributions_L3, n_iter=3, cv=3, scoring='accuracy', random_state=42, verbose=2)
random_search_L3_new.fit(x_train_trf_L3, y_train[L3])

print(f"Best parameters: {random_search_L3_new.best_params_}")
print(f"Best cross-validation score: {random_search_L3_new.best_score_}")
print(f"Best model: {random_search_L3_new.best_estimator_}")

best_params_L3 = random_search_L3_new.best_params_
best_model_L3 = random_search_L3_new.best_estimator_

y_pred = best_model_L3.predict(x_valid_trf_L3)
accuracy = accuracy_score(y_valid[L3], y_pred)
print(f'Accuracy on test data:{accuracy}')

y_pred_L3 = best_model_L3.predict(x_test_trf_L3)
y_pred_L3

"""Save the best model"""

import joblib

# Save the model to your Google Drive using joblib
joblib.dump(best_model_L3, f'{WORKING_DIR}/best_model_L3.joblib')

loaded_model = joblib.load(f'{WORKING_DIR}/best_model_L3.joblib')

y_pred_L3_load = loaded_model.predict(x_valid_trf_L3)
accuracy = accuracy_score(y_valid[L3], y_pred_L3_load)
print(f'Accuracy on test data: {accuracy}')

print (metrics.confusion_matrix(y_valid[L3], y_pred_L3_load))
print (metrics.accuracy_score(y_valid[L3], y_pred_L3_load))
print (metrics.precision_score(y_valid[L3], y_pred_L3_load, average="weighted"))
print (metrics.recall_score(y_valid[L3], y_pred_L3_load, average="weighted"))

# Create the base DataFrame
output_df_L3 = pd.DataFrame({
    'label_3': y_pred_L3_load
})

output_df_L3.head()

output_df_L3.to_csv(f"{WORKING_DIR}/L3.csv", index=False)

"""# Label 4"""

from sklearn import svm

clf_L4 = svm.SVC(kernel = 'linear', class_weight='balanced')
clf_L4.fit(x_train[L4], y_train[L4])

from sklearn import metrics

y_pred_L4 = clf_L4.predict(x_valid[L4])
y_pred_test_before_L4 = clf_L4.predict(x_test[L4])

print('Predicted labels before feature engineering:', y_pred_test_before_L4)

print (metrics.confusion_matrix(y_valid[L4], y_pred_L4))
print (metrics.accuracy_score(y_valid[L4], y_pred_L4))
print (metrics.precision_score(y_valid[L4], y_pred_L4, average="weighted"))
print (metrics.recall_score(y_valid[L4], y_pred_L4, average="weighted"))

"""## Feature Engineering"""

from sklearn.decomposition import PCA

pca_L4 = PCA(n_components=0.95, svd_solver='full')
pca_L4.fit(x_train[L4])
x_train_trf_L4 = pd.DataFrame(pca_L4.transform(x_train[L4]))
x_valid_trf_L4 = pd.DataFrame(pca_L4.transform(x_valid[L4]))

print("Shape after PCA: ", x_train_trf_L4.shape)

x_test_trf_L4= pd.DataFrame(pca_L4.transform(x_test[L4]))

clf_L4 = svm.SVC(kernel = 'linear', class_weight='balanced')
clf_L4.fit(x_train_trf_L4, y_train[L4])

y_pred_L4 = clf_L4.predict(x_valid_trf_L4)

print (metrics.confusion_matrix(y_valid[L4], y_pred_L4))
print (metrics.accuracy_score(y_valid[L4], y_pred_L4))
print (metrics.precision_score(y_valid[L4], y_pred_L4, average="weighted"))
print (metrics.recall_score(y_valid[L4], y_pred_L4, average="weighted"))

"""### PCA with Hyper-parameter Tuning and Cross Validation


"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score

# Define the model
svc_L4 = svm.SVC(class_weight='balanced')

# Parameters to search
param_distributions_L4 = {
    'C': np.logspace(-3, 3, 7),  # logarithmic scale from 0.001 to 1000
    'kernel': ['poly'],
    'gamma': np.logspace(-3, 3, 7)
}

"""**Wrong**"""

# Randomized search
random_search_L4 = RandomizedSearchCV(svc_L4, param_distributions_L4, n_iter=3, cv=5, scoring='accuracy', random_state=42, verbose=2, n_jobs=-1)
random_search_L4.fit(x_train[L4], y_train[L4])

print(f"Best parameters: {random_search_L4.best_params_}")
print(f"Best cross-validation score: {random_search_L4.best_score_}")
print(f"Best model: {random_search_L4.best_estimator_}")

best_params = random_search_L4.best_params_
best_model = random_search_L4.best_estimator_

y_pred = best_model.predict(x_valid[L4])
accuracy = accuracy_score(y_valid[L4], y_pred)
print(f'Accuracy on test data:{accuracy}')

y_pred_L4 = best_model.predict(x_test)

y_pred_L4

"""Correct"""

# Randomized search new
random_search_L4_new = RandomizedSearchCV(svc_L4, param_distributions_L4, n_iter=3, cv=3, scoring='accuracy', random_state=42, verbose=2)
random_search_L4_new.fit(x_train_trf_L4, y_train[L4])

print(f"Best parameters: {random_search_L4_new.best_params_}")
print(f"Best cross-validation score: {random_search_L4_new.best_score_}")
print(f"Best model: {random_search_L4_new.best_estimator_}")

best_params_L4 = random_search_L4_new.best_params_
best_model_L4 = random_search_L4_new.best_estimator_

y_pred = best_model_L4.predict(x_valid_trf_L4)
accuracy = accuracy_score(y_valid[L4], y_pred)
print(f'Accuracy on test data:{accuracy}')

y_pred_L4 = best_model_L4.predict(x_test_trf_L4)
y_pred_L4

"""Save the best model"""

import joblib

# Save the model to your Google Drive using joblib
joblib.dump(best_model_L4, f'{WORKING_DIR}/best_model_L4.joblib')

loaded_model = joblib.load(f'{WORKING_DIR}/best_model_L4.joblib')

y_pred_L4_load = loaded_model.predict(x_valid_trf_L4)
accuracy = accuracy_score(y_valid[L4], y_pred_L4_load)
print(f'Accuracy on test data: {accuracy}')

print (metrics.confusion_matrix(y_valid[L4], y_pred_L4_load))
print (metrics.accuracy_score(y_valid[L4], y_pred_L4_load))
print (metrics.precision_score(y_valid[L4], y_pred_L4_load, average="weighted"))
print (metrics.recall_score(y_valid[L4], y_pred_L4_load, average="weighted"))

# Create the base DataFrame
output_df_L4 = pd.DataFrame({
    'label_4': y_pred_L4_load
})

output_df_L4.head()

output_df_L4.to_csv(f"{WORKING_DIR}/L4.csv", index=False)

"""## Valid"""

y_pred_L1_valid = best_model_L1.predict(x_valid_trf_L1)
accuracy = accuracy_score(y_valid[L1], y_pred_L1)
print(f'Accuracy on test data: {accuracy}')

print (metrics.confusion_matrix(y_valid[L1], y_pred_L1))
print (metrics.accuracy_score(y_valid[L1], y_pred_L1))
print (metrics.precision_score(y_valid[L1], y_pred_L1, average="weighted"))
print (metrics.recall_score(y_valid[L1], y_pred_L1, average="weighted"))

y_pred_L2_valid = best_model_L2.predict(x_valid_trf_L2)
accuracy = accuracy_score(y_valid[L2], y_pred_L2)
print(f'Accuracy on test data: {accuracy}')

print (metrics.confusion_matrix(y_valid[L2], y_pred_L2))
print (metrics.accuracy_score(y_valid[L2], y_pred_L2))
print (metrics.precision_score(y_valid[L2], y_pred_L2, average="weighted"))
print (metrics.recall_score(y_valid[L2], y_pred_L2, average="weighted"))

y_pred_L3_valid = best_model_L3.predict(x_valid_trf_L3)
accuracy = accuracy_score(y_valid[L3], y_pred_L3)
print(f'Accuracy on test data: {accuracy}')

print (metrics.confusion_matrix(y_valid[L3], y_pred_L3))
print (metrics.accuracy_score(y_valid[L3], y_pred_L3))
print (metrics.precision_score(y_valid[L3], y_pred_L3, average="weighted"))
print (metrics.recall_score(y_valid[L3], y_pred_L3, average="weighted"))

y_pred_L4_valid = best_model_L4.predict(x_valid_trf_L4)
accuracy = accuracy_score(y_valid[L4], y_pred_L4)
print(f'Accuracy on test data: {accuracy}')

print (metrics.confusion_matrix(y_valid[L4], y_pred_L4))
print (metrics.accuracy_score(y_valid[L4], y_pred_L4))
print (metrics.precision_score(y_valid[L4], y_pred_L4, average="weighted"))
print (metrics.recall_score(y_valid[L4], y_pred_L4, average="weighted"))

"""# Output"""

# Create the base DataFrame
output_df_L1 = pd.DataFrame({
    'label_1': y_pred_L1
})

output_df_L1.head()

# Create the base DataFrame
output_df_L2 = pd.DataFrame({
    'label_2': y_pred_L2
})

output_df_L2.head()

# Create the base DataFrame
output_df_L3 = pd.DataFrame({
    'label_3': y_pred_L3
})

output_df_L3.head()

# Create the base DataFrame
output_df_L4 = pd.DataFrame({
    'label_4': y_pred_L4
})

output_df_L4.head()

# Save the DataFrame to the specified CSV file path
output_df_L1.to_csv(f"{WORKING_DIR}/L1.csv", index=False)
output_df_L2.to_csv(f"{WORKING_DIR}/L2.csv", index=False)
output_df_L3.to_csv(f"{WORKING_DIR}/L3.csv", index=False)
output_df_L4.to_csv(f"{WORKING_DIR}/L4.csv", index=False)